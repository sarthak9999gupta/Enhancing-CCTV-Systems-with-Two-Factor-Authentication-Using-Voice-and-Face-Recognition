{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Enhancing CCTV Systems with Two-Factor Authentication Using Voice and Face Recognition**\n",
        "\n",
        "## **Author: Sarthak**\n",
        "**Last Updated: 17 Nov 2024|1:10 Pm**\n",
        "\n"
      ],
      "metadata": {
        "id": "eZ7CD_G3Diod"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the implementation of a two-factor authentication (2FA) system for CCTV with face and voice recognition, the following Python libraries are crucial:\n",
        "\n",
        "- **`opencv-python`**: Used for image and video processing.\n",
        "- **`dlib`**: Provides facial landmark detection and face recognition.\n",
        "- **`face-recognition`**: Simplifies the process of identifying faces.\n",
        "- **`imutils`**: Offers utility functions for image operations.\n",
        "- **`scipy`**: Provides scientific and numerical computations.\n",
        "- **`requests`**: Facilitates HTTP requests for email notifications.\n",
        "- **`SpeechRecognition`**: Converts speech to text for voice authentication.\n",
        "- **`pydub`**: Allows audio file manipulation for voice analysis.\n",
        "\n",
        "These libraries ensure efficient face and voice recognition integration, crucial for enhanced security in 2FA systems."
      ],
      "metadata": {
        "id": "_bMqFo3jEVIf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlpzgrsmdDMC",
        "outputId": "032a5191-f8ac-475f-91d5-309a97d969a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.4)\n",
            "Requirement already satisfied: dlib in /usr/local/lib/python3.10/dist-packages (19.24.2)\n",
            "Collecting face-recognition\n",
            "  Downloading face_recognition-1.3.0-py2.py3-none-any.whl.metadata (21 kB)\n",
            "Collecting face-recognition-models>=0.3.0 (from face-recognition)\n",
            "  Downloading face_recognition_models-0.3.0.tar.gz (100.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.1/100.1 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.10/dist-packages (from face-recognition) (8.1.7)\n",
            "Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.10/dist-packages (from face-recognition) (19.24.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from face-recognition) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from face-recognition) (11.0.0)\n",
            "Downloading face_recognition-1.3.0-py2.py3-none-any.whl (15 kB)\n",
            "Building wheels for collected packages: face-recognition-models\n",
            "  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566162 sha256=8d9e83948eabeca06fe668a34db421f8371e15a3d9a6189166d1356c0d54d475\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/eb/cf/e9eced74122b679557f597bb7c8e4c739cfcac526db1fd523d\n",
            "Successfully built face-recognition-models\n",
            "Installing collected packages: face-recognition-models, face-recognition\n",
            "Successfully installed face-recognition-1.3.0 face-recognition-models-0.3.0\n",
            "Requirement already satisfied: imutils in /usr/local/lib/python3.10/dist-packages (0.5.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Collecting SpeechRecognition\n",
            "  Downloading SpeechRecognition-3.11.0-py2.py3-none-any.whl.metadata (28 kB)\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2024.8.30)\n",
            "Downloading SpeechRecognition-3.11.0-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, SpeechRecognition\n",
            "Successfully installed SpeechRecognition-3.11.0 pydub-0.25.1\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python\n",
        "\n",
        "!pip install dlib\n",
        "\n",
        "!pip install face-recognition\n",
        "\n",
        "!pip install imutils\n",
        "\n",
        "!pip install scipy\n",
        "\n",
        "!pip install requests\n",
        "\n",
        "!pip install SpeechRecognition pydub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this segment of code, the libraries dlib and face-recognition are managed using pip, Python's package manager. First, the face-recognition and dlib libraries are uninstalled using pip uninstall -y face-recognition dlib, ensuring any conflicting versions are removed. Then, dlib==19.18.0 is installed explicitly with the command pip install dlib==19.18.0 to ensure compatibility, as some versions may cause issues with the face-recognition library. Lastly, the face-recognition library is reinstalled to provide simplified face detection and recognition functionality for the two-factor authentication (2FA) system."
      ],
      "metadata": {
        "id": "K-rQVV6mElWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y face-recognition dlib\n",
        "\n",
        "!pip install dlib==19.18.0\n",
        "\n",
        "!pip install face-recognition"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Zbb5t09LKmY",
        "outputId": "49a8a32f-93ea-4c71-ff0d-d104371e27bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: face-recognition 1.3.0\n",
            "Uninstalling face-recognition-1.3.0:\n",
            "  Successfully uninstalled face-recognition-1.3.0\n",
            "Found existing installation: dlib 19.24.2\n",
            "Uninstalling dlib-19.24.2:\n",
            "  Successfully uninstalled dlib-19.24.2\n",
            "Collecting dlib==19.18.0\n",
            "  Downloading dlib-19.18.0.tar.gz (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: dlib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This segment of code demonstrates how to send an email with an image attachment using the Mailgun API. First, the necessary Mailgun API key, domain, and recipient's email are specified. The function send_email_with_attachment(image_path, subject) is responsible for sending an email with the attached image. It first prepares the email content and then opens the specified image in binary mode (rb) to attach it to the email. Using the requests library, the script sends a POST request to the Mailgun API to trigger the email. A success message is printed if the email is sent successfully, otherwise, an error message is displayed."
      ],
      "metadata": {
        "id": "GRskof_zEnc3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8n0rVnRsIWYP"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# Mailgun API setup\n",
        "MAILGUN_API_KEY = '3fa059db07c52e7e18d1cebd81e7bccf-f6fe91d3-b10b41ea'  # Replace with your Mailgun API key\n",
        "MAILGUN_DOMAIN = 'sandboxb43938edf367403fb9a18b0cb9dbfecd.mailgun.org'  # Replace with your Mailgun domain\n",
        "OWNER_EMAIL = 'arsheensingh100@gmail.com'  # Replace with the owner's email address\n",
        "\n",
        "# Email sending function with image attachment\n",
        "def send_email_with_attachment(image_path, subject=\"Unknown Visitor Detected\"):\n",
        "    # Mailgun API URL for sending emails\n",
        "    url = f\"https://api.mailgun.net/v3/{MAILGUN_DOMAIN}/messages\"\n",
        "\n",
        "    # Prepare email content\n",
        "    data = {\n",
        "        \"from\": \"CCTV System <noreply@yourdomain.com>\",  # Change the sender email if needed\n",
        "        \"to\": OWNER_EMAIL,\n",
        "        \"subject\": subject,\n",
        "        \"text\": \"An unknown visitor was detected by the CCTV system.\"\n",
        "    }\n",
        "\n",
        "    # Open image and prepare it for attachment\n",
        "    with open(image_path, \"rb\") as img_file:\n",
        "        files = {\n",
        "            \"attachment\": img_file\n",
        "        }\n",
        "\n",
        "        # Make the POST request to Mailgun's API to send the email\n",
        "        response = requests.post(\n",
        "            url,\n",
        "            auth=(\"api\", MAILGUN_API_KEY),\n",
        "            data=data,\n",
        "            files=files\n",
        "        )\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            print(\"Email sent successfully with attachment!\")\n",
        "        else:\n",
        "            print(f\"Error sending email: {response.status_code} - {response.text}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code, the setup for face recognition and eye-blink detection is configured. First, necessary libraries such as cv2, face_recognition, and dlib are imported. The function load_known_faces(path, name) is used to load the images of known family members, extract their face encodings, and store them in known_face_encodings and known_face_names. The dlib library's frontal face detector and shape predictor are initialized to detect facial landmarks, crucial for further processing like eye-blink detection. Additionally, a shape predictor model file (shape_predictor_68_face_landmarks.dat) is required to identify these landmarks.\n",
        "\n"
      ],
      "metadata": {
        "id": "bLaZ-TfOEsMB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yej6VaF655fo"
      },
      "outputs": [],
      "source": [
        "# 2. Face Recognition and Eye-Blink Detection Setup\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import face_recognition\n",
        "import dlib\n",
        "from scipy.spatial import distance\n",
        "import time\n",
        "\n",
        "# Load known faces\n",
        "known_face_encodings = []\n",
        "known_face_names = []\n",
        "\n",
        "def load_known_faces(path, name):\n",
        "    image = face_recognition.load_image_file(path)\n",
        "    encoding = face_recognition.face_encodings(image)[0]\n",
        "    known_face_encodings.append(encoding)\n",
        "    known_face_names.append(name)\n",
        "\n",
        "# Add known family members' faces\n",
        "load_known_faces(\"arsheen_photo.jpg\", \"Family Member 1\")\n",
        "# Add other known faces as needed\n",
        "#load_known_faces(\"family_member_2.jpg\", \"Family Member 2\")\n",
        "\n",
        "# Initialize facial landmarks detector and other necessary objects\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "dlib_shape_predictor_path = \"/content/shape_predictor_68_face_landmarks.dat\"  # Make sure this file is available\n",
        "predictor = dlib.shape_predictor(dlib_shape_predictor_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this code segment, video capture is initialized using OpenCV's cv2.VideoCapture() to load the video for processing. The video path is specified as /content/mayan video .mp4, which should be replaced with the correct path for your video file. The eye-blink detection parameters are set, including the eye_aspect_ratio_threshold (0.2) and eye_aspect_ratio_consec_frames (3), which define the criteria for detecting eye blinks. The function calculate_eye_aspect_ratio(eye_points) computes the eye aspect ratio (EAR) by calculating distances between specific eye landmarks. This ratio helps determine whether the eyes are closed, triggering blink detection if the threshold is exceeded.\n",
        "\n"
      ],
      "metadata": {
        "id": "w1QUdFkMEvdI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fWC--OWqzCO"
      },
      "outputs": [],
      "source": [
        "# 3. Video Capture and Eye-Blink Detection Parameters\n",
        "# Video capture initialization\n",
        "video_capture = cv2.VideoCapture(\"/content/mayan video .mp4\")  # Replace with your video path\n",
        "\n",
        "# Eye-blink detection parameters\n",
        "eye_aspect_ratio_threshold = 0.2\n",
        "eye_aspect_ratio_consec_frames = 3\n",
        "frame_counter = 0\n",
        "eye_blink_detected = False\n",
        "\n",
        "# Function to calculate eye aspect ratio\n",
        "def calculate_eye_aspect_ratio(eye_points):\n",
        "    A = distance.euclidean((eye_points[1].x, eye_points[1].y), (eye_points[5].x, eye_points[5].y))\n",
        "    B = distance.euclidean((eye_points[2].x, eye_points[2].y), (eye_points[4].x, eye_points[4].y))\n",
        "    C = distance.euclidean((eye_points[0].x, eye_points[0].y), (eye_points[3].x, eye_points[3].y))\n",
        "    return (A + B) / (2.0 * C)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code snippet, the function add_timestamp_to_frame(frame) adds a timestamp to the video frame. The timestamp is generated using the time.strftime() function, which formats the current time into a readable string in the format \"YYYY-MM-DD HH:MM\n",
        "\". The cv2.putText() method is then used to overlay the timestamp on the frame. The position and font size are set, and the color is defined as yellow (255, 255, 0). The Y-position of the timestamp is adjusted for visibility, and the text is drawn with a thickness of 2. This function ensures that each video frame has a timestamp, providing a real-time record for the CCTV system."
      ],
      "metadata": {
        "id": "z-S9IksfE5tG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wq30Bh_hrC6W"
      },
      "outputs": [],
      "source": [
        "def add_timestamp_to_frame(frame):\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")  # Get the current timestamp\n",
        "    text_y_position = 80  # Adjust Y-position for the timestamp\n",
        "    cv2.putText(frame, timestamp, (10, text_y_position), font, 0.8, (255, 255, 0), 2, cv2.LINE_AA)\n",
        "    return frame\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part of the code implements the main loop for face detection and eye-blink detection. It processes each video frame, detecting faces and comparing them to known family members' faces using the face_recognition library. When a face match is found, the system performs eye-blink detection by calculating the Eye Aspect Ratio (EAR) using the landmarks detected by the dlib library. If the EAR falls below a threshold, it indicates a blink. Upon detecting a blink, access is granted, and the frame is captured. If an unknown visitor is detected, an image is saved and sent to the owner via email using the Mailgun API.\n",
        "\n"
      ],
      "metadata": {
        "id": "z0pzFMuaFCF0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4Ni1M8erJLY",
        "outputId": "30757075-9fbc-4181-e46a-e17762b557b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unknown visitor detected, sending notification...\n",
            "Email sent successfully with attachment!\n"
          ]
        }
      ],
      "source": [
        "# 5. Main Loop for Face Detection and Blink Detection\n",
        "unknown_visitor_detected = False\n",
        "captured_frames = []\n",
        "\n",
        "# Main loop to process video frames\n",
        "while True:\n",
        "    ret, frame = video_capture.read()\n",
        "    if not ret:\n",
        "        print(\"Error: Failed to read a frame from the video.\")\n",
        "        break\n",
        "\n",
        "    rgb_frame = frame[:, :, ::-1]  # Convert frame from BGR to RGB\n",
        "    face_locations = face_recognition.face_locations(rgb_frame)\n",
        "    face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
        "\n",
        "    for face_encoding, face_location in zip(face_encodings, face_locations):\n",
        "        matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
        "        name = \"Unknown\"\n",
        "\n",
        "        if True in matches:\n",
        "            first_match_index = matches.index(True)\n",
        "            name = known_face_names[first_match_index]\n",
        "\n",
        "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "            rects = detector(gray, 0)\n",
        "\n",
        "            for rect in rects:\n",
        "                shape = predictor(gray, rect)\n",
        "                left_eye = [shape.part(i) for i in range(36, 42)]\n",
        "                right_eye = [shape.part(i) for i in range(42, 48)]\n",
        "\n",
        "                left_ear = calculate_eye_aspect_ratio(left_eye)\n",
        "                right_ear = calculate_eye_aspect_ratio(right_eye)\n",
        "                ear = (left_ear + right_ear) / 2.0\n",
        "\n",
        "                if ear < eye_aspect_ratio_threshold:\n",
        "                    frame_counter += 1\n",
        "                else:\n",
        "                    if frame_counter >= eye_aspect_ratio_consec_frames:\n",
        "                        eye_blink_detected = True\n",
        "                    frame_counter = 0\n",
        "\n",
        "                if eye_blink_detected:\n",
        "                    print(f\"Blink Detected! Access granted to {name}\")\n",
        "\n",
        "                    for point in left_eye + right_eye:\n",
        "                        cv2.circle(frame, (point.x, point.y), 2, (0, 255, 0), -1)\n",
        "                    cv2.putText(frame, \"Blink Detected\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "                    frame = add_timestamp_to_frame(frame)\n",
        "\n",
        "                    captured_frames.append(frame)\n",
        "                    if len(captured_frames) >= 3:  # Capture 3 frames after blink detection\n",
        "                        break\n",
        "\n",
        "        else:\n",
        "            # If an unknown visitor is detected, capture image and send email\n",
        "            if not unknown_visitor_detected:\n",
        "                print(\"Unknown visitor detected, sending notification...\")\n",
        "                cv2.imwrite(\"/content/unknown_visitor.jpg\", frame)  # Save the captured frame\n",
        "                send_email_with_attachment(\"/content/unknown_visitor.jpg\")  # Send email with attachment\n",
        "                unknown_visitor_detected = True\n",
        "                break\n",
        "\n",
        "    if eye_blink_detected or len(captured_frames) >= 3 or unknown_visitor_detected:\n",
        "        break  # Stop processing after either condition is met\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section of the code handles speech recognition for voice-based access control as part of the two-factor authentication process. It uses the speech_recognition library to convert audio into text by utilizing the Google Web Speech API. The function audio_to_text processes the audio file and returns the recognized speech as text, which is then checked against a predefined password (\"open\"). If the recognized text matches the password, access is granted. Otherwise, access is denied. This ensures that only authorized users can gain access after successfully completing the face recognition and eye-blink detection phases."
      ],
      "metadata": {
        "id": "EKk3LzRWFHkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import speech_recognition as sr\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# Step 3: Define Constants\n",
        "PASSWORD = \"open\"  # Set your password here\n",
        "\n",
        "# Function to convert audio to text\n",
        "def audio_to_text(audio_file):\n",
        "    recognizer = sr.Recognizer()\n",
        "    with sr.AudioFile(audio_file) as source:\n",
        "        audio_data = recognizer.record(source)\n",
        "    try:\n",
        "        # Recognize speech using Google Web Speech API\n",
        "        text = recognizer.recognize_google(audio_data)\n",
        "        return text.lower()  # Convert to lowercase for comparison\n",
        "    except sr.UnknownValueError:\n",
        "        print(\"Could not understand audio\")\n",
        "        return None\n",
        "    except sr.RequestError as e:\n",
        "        print(f\"Could not request results from Google Speech Recognition service; {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to check access\n",
        "def check_access(audio_file):\n",
        "    recognized_text = audio_to_text(audio_file)\n",
        "    if recognized_text is not None:\n",
        "        print(f\"Recognized Text: {recognized_text}\")\n",
        "        if recognized_text == PASSWORD:\n",
        "            print(\"Access Granted!\")\n",
        "        else:\n",
        "            print(\"Access Denied!\")\n",
        "    else:\n",
        "        print(\"No valid speech recognized.\")\n",
        "\n",
        "# Step 4: Upload your test audio file (WAV format)\n",
        "#from google.colab import files\n",
        "#uploaded = files.upload()\n",
        "\n",
        "# Assuming the uploaded file is named 'test_audio.wav'\n",
        "test_audio_file = '/content/drive/MyDrive/Two factor authentication/Arsheen/WhatsApp Audio 2024-11-08 at 3.57.43 PM.wav'  # Change this if your file has a different name\n",
        "\n",
        "# Step 5: Check Access with the provided audio file\n",
        "check_access(test_audio_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rU8W_djDR7qX",
        "outputId": "fcafd515-09cc-4e0d-dca0-8543856a2485"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recognized Text: hello open\n",
            "Access Denied!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section of the code processes the captured frames after the successful detection of a blink or an unknown visitor. The frames are saved as images in the specified directory (/content/blink_frame_{idx}.jpg) and are displayed one by one using cv2_imshow for visual verification in the Colab environment. The video capture is then released, and all OpenCV windows are closed using cv2.destroyAllWindows to free up resources and finalize the process. These final steps ensure that the system not only captures frames when necessary but also allows for the user to view and save them."
      ],
      "metadata": {
        "id": "o0J2ESAJFI3q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTgGWfvPrN5t"
      },
      "outputs": [],
      "source": [
        "# 6. Final Steps: Display and Save Captured Frames\n",
        "# Process and display captured frames (optional)\n",
        "for idx, captured_frame in enumerate(captured_frames):\n",
        "    cv2.imwrite(f\"/content/blink_frame_{idx}.jpg\", captured_frame)  # Save captured frames\n",
        "    cv2_imshow(captured_frame)  # Display captured frames in Colab\n",
        "\n",
        "# Release video capture and close windows\n",
        "video_capture.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    }
  ]
}